{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1. Define hyperparameters\n",
    "# -------------------------------------------------------------\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "NUM_CLASSES = 7         # Number of emotion categories\n",
    "BATCH_SIZE = 32         # Try different batch sizes (e.g., 16, 32, 64)\n",
    "EPOCHS_STAGE1 = 10      # Number of epochs for stage 1 (freeze backbone)\n",
    "EPOCHS_STAGE2 = 10      # Number of epochs for stage 2 (fine-tuning)\n",
    "LEARNING_RATE_STAGE1 = 1e-4\n",
    "LEARNING_RATE_STAGE2 = 1e-5\n",
    "\n",
    "# Dataset directory structure (adjust based on actual data location)\n",
    "TRAIN_DIR = \"./archive/train\"\n",
    "VAL_DIR = \"./archive/test\"  # Or \"./archive/val\" depending on dataset setup\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2. Data augmentation and generators\n",
    "# -------------------------------------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=10,            # Small rotation\n",
    "    width_shift_range=0.1,        # Horizontal shift\n",
    "    height_shift_range=0.1,       # Vertical shift\n",
    "    brightness_range=(0.9, 1.1),  # Adjust brightness\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=TRAIN_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory=VAL_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3. Build pre-trained model (MobileNetV2)\n",
    "# -------------------------------------------------------------\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',       # Use ImageNet pre-trained weights\n",
    "    include_top=False,        # Exclude the top classification layer\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    ")\n",
    "\n",
    "# Freeze all convolutional layers (only train the custom classification head in stage 1)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4. Build custom classification head\n",
    "# -------------------------------------------------------------\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5. Compile the model - Using Precision as a metric\n",
    "# -------------------------------------------------------------\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE_STAGE1),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.metrics.Precision(name='precision')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 6. Define callbacks\n",
    "# -------------------------------------------------------------\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,         # Reduce learning rate factor\n",
    "        patience=3,         # Reduce LR after 3 epochs with no improvement\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,         # Stop training after 5 epochs with no improvement\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 7. Stage 1 training (train only the classification head)\n",
    "# -------------------------------------------------------------\n",
    "print(\"=== Stage 1: Training top layers (base_model frozen) ===\")\n",
    "history_stage1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_STAGE1,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 8. Stage 2 fine-tuning - Unfreeze last 20 layers\n",
    "# -------------------------------------------------------------\n",
    "print(\"=== Stage 2: Fine-tuning some layers of base_model ===\")\n",
    "# Unfreeze the last 20 layers\n",
    "for layer in base_model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate and keep Precision as the metric\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=LEARNING_RATE_STAGE2),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tf.keras.metrics.Precision(name='precision')]\n",
    ")\n",
    "\n",
    "history_stage2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS_STAGE2,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 9. Evaluate and save the model\n",
    "# -------------------------------------------------------------\n",
    "test_loss, test_precision = model.evaluate(val_generator, verbose=1)\n",
    "print(f\"Final Test Loss: {test_loss:.4f} | Test Precision: {test_precision:.4f}\")\n",
    "\n",
    "model.save(\"emotion_recognition_transfer_precision.h5\")\n",
    "print(\"Model saved as emotion_recognition_transfer_precision.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
